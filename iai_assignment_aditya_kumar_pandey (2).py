# -*- coding: utf-8 -*-
"""IAI Assignment Aditya Kumar Pandey.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VhncKfo89mALCuoFoIzJGAl3kf6Kia9x
"""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Path to your project folder
base_path = "/content/drive/MyDrive/invoice_reimbursement_system/data"

#Sub-paths
policy_path = os.path.join(base_path, "policy.pdf")
invoices_path = os.path.join(base_path, "invoices")

print("Policy PDF Path:", policy_path)
print("Invoices Folder Path:", invoices_path)



!pip install pdfplumber

import pdfplumber
import os

# List all invoice PDFs
invoice_files = [f for f in os.listdir(invoices_path) if f.endswith(".pdf")]

print("Found Invoice PDFs:")
for file in invoice_files:
    print("-", file)

def extract_text_from_pdf(pdf_path):
    full_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            full_text += page.extract_text() + "\n"
    return full_text

# Extract policy text
policy_text = extract_text_from_pdf(policy_path)
print("âœ… Extracted Policy Text (first 4000 chars):\n")
print(policy_text[:4000])  # Preview

# Dictionary to store invoice text
invoice_texts = {}

for file_name in invoice_files:
    file_path = os.path.join(invoices_path, file_name)
    text = extract_text_from_pdf(file_path)
    invoice_texts[file_name] = text

print(f"\nâœ… Extracted text from {len(invoice_texts)} invoice PDFs.")
print("\nSample Invoice Text (first 500 chars):\n")
print(invoice_texts[invoice_files[0]][:500])  # Preview

# Save texts to local files (in Colab)
with open("policy_text.txt", "w") as f:
    f.write(policy_text)

for fname, content in invoice_texts.items():
    with open(f"invoice_{fname}.txt", "w") as f:
        f.write(content)

# Save all extracted invoice texts as individual .txt files
os.makedirs("extracted_invoices", exist_ok=True)

for fname, content in invoice_texts.items():
    file_name = os.path.splitext(fname)[0] + ".txt"
    with open(os.path.join("extracted_invoices", file_name), "w") as f:
        f.write(content)

print("Saved extracted invoice texts to /extracted_invoices/")





!pip install -q google-generativeai

import os
import google.generativeai as genai

# Set API key correctly
os.environ["GEMINI_API_KEY"] = "AIzaSyAgmicbSK8OpFlmOMNPD0bWW9RYBSfIR50"

# Configure Gemini SDK
genai.configure(api_key=os.environ["GEMINI_API_KEY"])

# Create model instance
model = genai.GenerativeModel("gemini-1.5-flash")  # You can also try "gemini-pro" if needed

# Example usage
def analyze_invoice_with_policy(policy_text, invoice_text):
    prompt = f"""
You are a reimbursement policy expert.
Policy: {policy_text}

Invoice:
{invoice_text}

Return exactly:
Reimbursement Status: <Fully Reimbursed | Partially Reimbursed | Declined>
Reason: <One-sentence justification>
"""
    response = model.generate_content(prompt)
    return response.text.strip()

# Test
result = analyze_invoice_with_policy(
    "Meals under â‚¹1000 are reimbursable.",
    "Invoice for a business dinner totaling â‚¹1200."
)
print(result)



import os
import json
import threading
import pdfplumber
import google.generativeai as genai

# -- Setup Google Gemini --
os.environ["GEMINI_API_KEY"] = "AIzaSyAgmicbSK8OpFlmOMNPD0bWW9RYBSfIR50"
genai.configure(api_key=os.environ["GEMINI_API_KEY"])
model = genai.GenerativeModel("gemini-1.5-flash")

# -- PDF Text Extraction --
def extract_text_from_pdf(pdf_path):
    full_text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            full_text += page.extract_text() + "\n"
    return full_text

# -- Load Policy --
policy_path = "/content/drive/MyDrive/invoice_reimbursement_system/data/policy.pdf"
policy_text = extract_text_from_pdf(policy_path)
print(" Extracted Policy Text\n")

# -- Load Invoices --
invoices_path = "/content/drive/MyDrive/invoice_reimbursement_system/data/invoices"
invoice_files = os.listdir(invoices_path)
invoice_texts = {
    file_name: extract_text_from_pdf(os.path.join(invoices_path, file_name))
    for file_name in invoice_files
}
print(f" Extracted text from {len(invoice_texts)} invoice PDFs.")

# -- Gemini Analysis Function --
def analyze_invoice(file_name, invoice_text, results_dict):
    try:
        prompt = f"""
You are a reimbursement policy expert.

Policy:
{policy_text}

Invoice:
{invoice_text}

Return exactly:
Reimbursement Status: <Fully Reimbursed | Partially Reimbursed | Declined>
Reason: <One-sentence justification>
"""
        response = model.generate_content(prompt)
        results_dict[file_name] = response.text.strip()
        print(f"{file_name} analyzed.")
    except Exception as e:
        print(f" {file_name} failed: {e}")
        results_dict[file_name] = f"Error: {str(e)}"

# -- Run Analysis with Threads --
results = {}
threads = []

for file_name, text in invoice_texts.items():
    t = threading.Thread(target=analyze_invoice, args=(file_name, text, results))
    threads.append(t)
    t.start()

for t in threads:
    t.join()

# -- Save Results --
with open("invoice_analysis_results.json", "w") as f:
    json.dump(results, f, indent=2)

print("\n Results saved to invoice_analysis_results.json")



pip install chromadb

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction

# Load local model
embedding_function = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")

# Imports
import os
import json
import chromadb
from sentence_transformers import SentenceTransformer
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction

# Load local embedding model
embedding_function = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")

# Paths
invoices_path = "/content/drive/MyDrive/invoice_reimbursement_system/data/invoices"
results_path = "/content/invoice_analysis_results.json"
export_path = "/content/invoice_embeddings_export.json"  # File to save embedded data

# Load results
with open(results_path, "r") as f:
    analysis_results = json.load(f)

# Setup ChromaDB with SentenceTransformer
client = chromadb.Client()
collection = client.get_or_create_collection(name="invoice_embeddings", embedding_function=embedding_function)

# Helper to extract metadata
def extract_metadata(file_name, result_text):
    lines = result_text.splitlines()
    status = reason = ""
    for line in lines:
        if "Reimbursement Status" in line:
            status = line.split(":")[1].strip()
        elif "Reason" in line:
            reason = line.split(":", 1)[1].strip()
    return {
        "file_name": file_name,
        "employee_name": file_name.split("_")[0],  # e.g., Rahul_invoice1.pdf â†’ Rahul
        "status": status,
        "reason": reason,
        "invoice_date": "2024-01-01"
    }

# Store all vectors and collect for export
export_data = []

for file_name in os.listdir(invoices_path):
    file_path = os.path.join(invoices_path, file_name)
    with open(file_path, "rb") as f:
        text = f.read().decode(errors="ignore")

    if file_name in analysis_results:
        result_text = analysis_results[file_name]
        metadata = extract_metadata(file_name, result_text)

        vector_input = text + "\n" + result_text
        collection.add(
            documents=[vector_input],
            metadatas=[metadata],
            ids=[file_name]
        )

        # Save for export
        export_data.append({
            "id": file_name,
            "text": vector_input,
            "metadata": metadata
        })

# Write export to file
with open(export_path, "w") as f:
    json.dump(export_data, f, indent=2)

print(" All invoices embedded and stored in ChromaDB.")
print(f"Embeddings also exported to: {export_path}")



# Imports
import chromadb
from sentence_transformers import SentenceTransformer
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction

# Load model and initialize DB
embedding_function = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
client = chromadb.Client()
collection = client.get_or_create_collection(name="invoice_embeddings", embedding_function=embedding_function)

def search_invoices(query, filters=None, top_k=5):
    """
    Perform vector similarity search on invoice embeddings with optional metadata filtering.

    Args:
        query (str): Search query in natural language.
        filters (dict, optional): Metadata filters. E.g., {"employee_name": "Rahul", "status": "Rejected"}.
        top_k (int): Number of top results to retrieve.

    Returns:
        list of str: Markdown-formatted results.
    """
    query_args = {
        "query_texts": [query],
        "n_results": top_k
    }

    # Format filters correctly using ChromaDB's syntax
    if filters:
        if len(filters) == 1:
            query_args["where"] = filters  # Single filter is valid
        else:
            # Wrap multiple filters in $and
            and_filters = [{k: v} for k, v in filters.items()]
            query_args["where"] = {"$and": and_filters}

    # Perform the query
    results = collection.query(**query_args)

    # Format as markdown
    formatted_results = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        md = f"###  Invoice: `{meta.get('file_name', 'Unknown')}`\n"
        md += f"-  **Employee**: {meta.get('employee_name', 'Unknown')}\n"
        md += f"-  **Date**: {meta.get('invoice_date', 'Unknown')}\n"
        md += f"-  **Status**: {meta.get('status', 'Unknown')}\n"
        md += f"-  **Reason**: {meta.get('reason', 'Not specified')}\n"
        md += f"\n---\n**Snippet:**\n```\n{doc[:500]}...\n```\n"
        formatted_results.append(md)

    return formatted_results


# Example 1: Query without filters
results = search_invoices("Why was Rahul's cab reimbursement partially rejected?")
for r in results:
    print(r)

# Example 2: Query with metadata filters
results = search_invoices(
    "cab invoices",
    filters={"employee_name": "Rahul", "status": "Partially Reimbursed"}
)
for r in results:
    print(r)



pip install transformers sentence-transformers chromadb



# Imports
import chromadb
from sentence_transformers import SentenceTransformer
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from transformers import pipeline

# Initialize SentenceTransformer for embeddings
embedding_function = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
client = chromadb.Client()
collection = client.get_or_create_collection(
    name="invoice_embeddings", embedding_function=embedding_function
)

# Vector Search Function (already implemented)
def search_invoices(query, filters=None, top_k=5):
    query_args = {
        "query_texts": [query],
        "n_results": top_k
    }
    if filters:
        query_args["where"] = {"$and": [{"employee_name": filters["employee_name"]}, {"status": filters["status"]}]}

    results = collection.query(**query_args)

    formatted_results = []
    for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
        md = f"###  Invoice: `{meta['file_name']}`\n"
        md += f"- **Employee**: {meta['employee_name']}\n"
        md += f"- **Date**: {meta['invoice_date']}\n"
        md += f"- **Status**: {meta['status']}\n"
        md += f"- **Reason**: {meta['reason']}\n"
        md += f"\n---\n**Snippet:**\n```\n{doc[:500]}...\n```\n"
        formatted_results.append(md)

    return formatted_results

# âœ… Local HuggingFace RAG Pipeline
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-base")

def rag_chatbot_local(query, top_k=3):
    """
    Local Retrieval-Augmented Generation (RAG) chatbot using Hugging Face's flan-t5-base.
    """
    retrieved_docs = search_invoices(query, top_k=top_k)
    context = "\n\n".join(retrieved_docs)

    prompt = f"""You are an expert in invoice reimbursement policies.
Based on the invoice data below, answer the user's question briefly.

--- INVOICE DATA START ---
{context}
--- INVOICE DATA END ---

Question: {query}
Answer:"""

    result = qa_pipeline(prompt, max_new_tokens=200)
    return result[0]['generated_text']

# âœ… Example usage
query = "Why was Rahul's cab reimbursement partially rejected?"
answer = rag_chatbot_local(query)
print("ðŸ’¬ Answer:\n", answer)



!pip install fastapi uvicorn nest-asyncio pyngrok chromadb sentence-transformers transformers



#new FASTAPI setup

# âœ… Step 1: Add Ngrok Auth Token
!ngrok config add-authtoken 2yMN4GMKS6s6c4qH5iSCm2ieiIR_3KGhaEMmUbndyr7ZjVPe4

from huggingface_hub import login

# Paste your Hugging Face token here
login("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")

from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from pyngrok import ngrok
import nest_asyncio
import uvicorn
import shutil
import os

#  Step 3: Load RAG/LLM Components
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from chromadb import Client
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction

# Load embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Load Mistral model
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1", device_map="auto", trust_remote_code=True)
model.eval()

#  ChromaDB
embedding_function = SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
client = Client()
collection = client.get_or_create_collection(name="invoice_embeddings", embedding_function=embedding_function)

# storage function
def embed_and_store(text, metadata):
    collection.add(documents=[text], metadatas=[metadata], ids=[metadata["filename"]])

# search function
def search_similar(query, top_k=1):
    results = collection.query(query_texts=[query], n_results=top_k)
    docs = results["documents"][0]
    metas = results["metadatas"][0]
    return [{"text": docs[i], "metadata": metas[i]} for i in range(len(docs))]

# Same answer generator
def generate_answer(context, query):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    output = model.generate(**inputs, max_new_tokens=100)
    return tokenizer.decode(output[0], skip_special_tokens=True).split("Answer:")[-1].strip()

# Step 4: FastAPI App
app = FastAPI()

@app.post("/upload/")
async def upload_file(file: UploadFile = File(...)):
    contents = await file.read()
    text = contents.decode("utf-8")

    embed_and_store(text, {"filename": file.filename})
    return {"status": "File processed and embedded", "filename": file.filename}

@app.post("/ask/")
async def ask_question(query: str):
    results = search_similar(query)
    if not results:
        return JSONResponse(status_code=400, content={"error": "No relevant documents found."})

    context = results[0]["text"]
    answer = generate_answer(context, query)
    return {"query": query, "answer": answer, "source": results[0]["metadata"]}

# Nest asyncio loop
nest_asyncio.apply()

# Kill old tunnels if any
ngrok.kill()

# Open ngrok tunnel to the FastAPI app
public_url = ngrok.connect(8000)
print(f"ðŸš€ Your public FastAPI URL: {public_url}")
print(f"ðŸ“„ Swagger Docs UI: {public_url}/docs")

# Run FastAPI app in current process (Colab-friendly)
uvicorn.run(app, host="0.0.0.0", port=8000)

# ðŸ‘‰ https://42eb-34-71-196-200.ngrok-free.app/docs





























