# Invoice-Reimbursement-System

## ğŸ“Œ Project Overview

This project is a full-stack solution to analyze a batch of invoice PDF files against a given insurance policy document. It leverages **Gemini Flash**, **ChromaDB**, **Mistral 7B**, and **FastAPI** to create a Retrieval-Augmented Generation (RAG) pipeline.This project implements a Retrieval-Augmented Generation (RAG) chatbot to answer questions related to invoice reimbursements. It uses semantic search using ChromaDB and SentenceTransformers, and generates natural language answers using transformerbased LLMs, including Flan-T5 and Mistral 7B.

---

## âœ… Phase 1: PDF Parsing & Gemini Flash-based Invoiceâ€“Policy Analysis

### ğŸ”§ Tools & Technologies:
- `PyMuPDF (fitz)` â€“ for reading and parsing PDF content.
- `Gemini Flash API` â€“ for quick and intelligent invoice-to-policy comparison.
- `Threading` â€“ to parallelize and accelerate the analysis of multiple invoices.

### ğŸ“Œ Description:
- The policy PDF is parsed and cleaned for semantic clarity.
- Each invoice is parsed and its textual content extracted.
- A composite prompt (invoice + policy) is sent to **Gemini Flash** for compliance analysis.
- Multithreading is used to process multiple invoices concurrently.
- The results for each invoice (compliance status and reasoning) are stored and later printed or saved.

---

## âœ… Phase 2: Vector Embedding with ChromaDB

### ğŸ”§ Tools & Technologies:
- `SentenceTransformers` â€“ for generating document embeddings using `"all-MiniLM-L6-v2"`.
- `ChromaDB` â€“ a lightweight, serverless vector store to index and query documents.

### ğŸ“Œ Description:
- Each invoice (in `.txt` format) is embedded using MiniLM.
- Metadata (e.g., filename) is stored alongside the document in ChromaDB.
- Top-k relevant documents are retrieved via cosine similarity during queries.

---

âœ… Phase 3: Conversational Query Engine

ğŸ¯ Objective

Developed a local Conversational Query System that allows users to interact with invoice reimbursement data using natural language questions. The system returns context-aware, policy-aligned answers by combining vector similarity search and Retrieval-Augmented Generation (RAG).

ğŸ§° Tools & Libraries Used

ChromaDB: Lightweight, local vector database for similarity search.

Sentence-Transformers: Used for embedding invoice and analysis data (all-MiniLM-L6-v2).

Hugging Face Transformers: Deployed google/flan-t5-base for fast, offline text generation.

ğŸ§© Key Components

3.1 Embedding Generation & Storage

Generated embeddings using the all-MiniLM-L6-v2 model.

Stored documents in ChromaDB under a collection named invoice_embeddings.

3.2 Semantic Search Function
Searches invoice documents based on user query with optional metadata filtering (e.g., only retrieve documents for a certain employee or status).

3.3 Local RAG Pipeline
Integrates retrieved invoice documents into a context-aware prompt and uses a local LLM (flant5-base) to answer questions. 

Retrieved top-k relevant invoices.

Formatted them into a prompt.

Passed to local Flan-T5-base model to generate answers.

example:
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-base")
query = "Why was Rahul's cab reimbursement partially rejected?"
answer = rag_chatbot_local(query)

âœ… Example Output:

Rahul's cab reimbursement was partially rejected because it exceeded the policy cap for airport drops. The approved limit was â‚¹600, but the submitted invoice was â‚¹1200.


## âœ… Phase 4: FastAPI Public API Deployment

### ğŸ”§ Tools & Technologies:
- `FastAPI` â€“ to define RESTful endpoints for uploading files and querying documents.
- `Uvicorn` â€“ as the ASGI server for local/cloud deployment.
- `Pyngrok` â€“ to tunnel the local server and expose it publicly (for demo/testing).
- `Mistral 7B Instruct` â€“ a locally loaded LLM used to answer questions based on document context.
- `Nest AsyncIO` â€“ to handle async event loop in Jupyter/Colab environments.

### ğŸ“Œ API Endpoints:
- **`/upload/`**: Accepts a `.txt` invoice file, embeds it, and stores it in ChromaDB.
- **`/ask/`**: Accepts a user query, fetches the most relevant invoice, and uses Mistral to generate an answer from its context.

---

## âš™ï¸ Example Workflow:

1. Upload an invoice (`.txt` format) to `/upload/`.
2. Ask a question like:  
   _â€œWas the blood test billed in accordance with the policy limits?â€_
3. The system retrieves the best-matching invoice using ChromaDB.
4. The Mistral LLM generates a contextualized, human-like answer based on that invoice.

---

## ğŸ§  Challenges Faced & Solutions

### â— Challenges:
- **LLM/API Dilemma**: Selecting the right LLM to analyze invoice content against policy rules was hard. While OpenAI models performed well, they were paid. Free models were either code-centric or underperformed on invoice analysis.
- **Processing Delays**: Analyzing 28 PDF invoices sequentially was too slow. Many free APIs imposed context or rate limits.
- **Quota Limits on Gemini Flash**: Although Gemini Flash performed fast and efficiently, its free-tier quota restricted full batch processing in one run.

### âœ… Solutions:
- Switched to **Gemini Flash**, which processed the invoiceâ€“policy comparison much faster and more intelligently than other free options.
- Used **threading** to process multiple invoices in parallel, significantly reducing overall runtime.
- For scalable querying, moved to a **RAG pipeline** using ChromaDB and **Mistral 7B**, thus bypassing LLM quota constraints for querying.

---

## ğŸš€ Future Enhancements

- Support uploading raw PDF files via FastAPI instead of pre-converted `.txt`.
- Replace Gemini Flash with a self-hosted model like **LLaMA 3** for complete control.
- Add user-friendly frontend (e.g., Streamlit or React).
- Integrate full authentication and rate-limiting on the API.
- Enable batch uploading and querying of multiple invoices at once.

---

